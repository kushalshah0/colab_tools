{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMm4YxVhNOEZsxfIMyOojMW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushalshah0/colab_tools/blob/main/ollama_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13eda400"
      },
      "source": [
        "### 1. Install Zstandard (zstd) Dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9cc06f6"
      },
      "source": [
        "print('Installing zstd dependency...')\n",
        "!sudo apt-get update && sudo apt-get install -y zstd\n",
        "print('zstd installed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1d328f5"
      },
      "source": [
        "### 2. Install Ollama"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d60af2d4"
      },
      "source": [
        "print('Installing Ollama...')\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "print('Ollama installation script executed. Verifying installation.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b27ef43e"
      },
      "source": [
        "### 3. Install PCI Utilities for GPU Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "108f7961"
      },
      "source": [
        "print('Installing pciutils for GPU detection...')\n",
        "!sudo apt-get install -y pciutils\n",
        "print('pciutils installed. Now we need to restart Ollama for it to recognize the GPU.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99f28056"
      },
      "source": [
        "### 4. Start Ollama Server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a87594ed"
      },
      "source": [
        "print('Starting Ollama server with GPU detection enabled...')\n",
        "import os\n",
        "os.environ['OLLAMA_HOST'] = '0.0.0.0'\n",
        "get_ipython().system_raw('ollama serve &') # Start Ollama server in the background\n",
        "print('Ollama server started. It should now detect GPU if available.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "745b7255"
      },
      "source": [
        "### 5. Pull Code-Generation Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd327250"
      },
      "source": [
        "print('Attempting to pull qwen2.5-coder:7b-instruct-q4_K_M model...')\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Give Ollama a moment to fully start after setting OLLAMA_HOST\n",
        "time.sleep(10)\n",
        "\n",
        "# Attempt to pull the primary model\n",
        "!ollama pull qwen2.5-coder:7b-instruct-q4_K_M\n",
        "\n",
        "# Check if the primary model was pulled successfully\n",
        "try:\n",
        "    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, check=True)\n",
        "    ollama_list_output = result.stdout\n",
        "    print(ollama_list_output)\n",
        "\n",
        "    if 'qwen2.5-coder:7b-instruct-q4_K_M' in ollama_list_output:\n",
        "        print('qwen2.5-coder:7b-instruct-q4_K_M model found. No fallback needed.')\n",
        "    else:\n",
        "        print('qwen2.5-coder:7b-instruct-q4_K_M model not found. Proceeding to pull fallback model deepseek-coder:6.7b-instruct-q4_K_M.')\n",
        "        !ollama pull deepseek-coder:6.7b-instruct-q4_K_M\n",
        "        print('deepseek-coder:6.7b-instruct-q4_K_M pull attempt completed. Please check output for success or failure.')\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error executing 'ollama list': {e.stderr}\")\n",
        "    print('Could not verify primary model, proceeding to pull fallback model deepseek-coder:6.7b-instruct-q4_K_M.')\n",
        "    !ollama pull deepseek-coder:6.7b-instruct-q4_K_M\n",
        "    print('deepseek-coder:6.7b-instruct-q4_K_M pull attempt completed. Please check output for success or failure.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96df27ea"
      },
      "source": [
        "### 6. Install Cloudflared"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fa4a1e3"
      },
      "source": [
        "print('Installing cloudflared...')\n",
        "!curl -L --output cloudflared.deb https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && sudo dpkg -i cloudflared.deb\n",
        "print('cloudflared installation initiated.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4855ea55"
      },
      "source": [
        "### 7. Create Cloudflare Tunnel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9d019da"
      },
      "source": [
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "print('Creating Cloudflare tunnel for Ollama...')\n",
        "output_file = 'cloudflared_tunnel_output.txt'\n",
        "\n",
        "# Run cloudflared tunnel in the background, redirecting output to a file\n",
        "# Using '2>&1' to redirect stderr to stdout, so all messages including the URL are captured\n",
        "get_ipython().system_raw(f'cloudflared tunnel --url http://localhost:11434 > {output_file} 2>&1 &')\n",
        "\n",
        "print('Waiting for tunnel to establish and URL to be generated...')\n",
        "time.sleep(10) # Give some time for the tunnel to establish and print the URL\n",
        "\n",
        "public_url = None\n",
        "if os.path.exists(output_file):\n",
        "    with open(output_file, 'r') as f:\n",
        "        output_content = f.read()\n",
        "    print(f'Cloudflared output:\\n{output_content}')\n",
        "\n",
        "    # Regex to find the URL in the cloudflared output\n",
        "    # The URL typically starts with 'https://' and ends with '.trycloudflare.com'\n",
        "    match = re.search(r'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com', output_content)\n",
        "    if match:\n",
        "        public_url = match.group(0)\n",
        "\n",
        "if public_url:\n",
        "    print(f'\\n!!! Your Ollama Public URL is: {public_url} !!!')\n",
        "    os.environ['OLLAMA_PUBLIC_URL'] = public_url # Store for later use\n",
        "else:\n",
        "    print('\\nError: Could not find the public Cloudflare tunnel URL in the output.')\n",
        "    print('Please check the output above for any errors or try running the command manually.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17c515cf"
      },
      "source": [
        "### 8. Verify Ollama API via Tunnel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ccfb0a"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "print('Verifying Ollama API via Cloudflare tunnel...')\n",
        "\n",
        "public_url = os.environ.get('OLLAMA_PUBLIC_URL')\n",
        "\n",
        "if not public_url:\n",
        "    print('Error: OLLAMA_PUBLIC_URL not found in environment variables.')\n",
        "else:\n",
        "    print(f'Using public URL: {public_url}')\n",
        "    api_endpoint = f'{public_url}/api/generate'\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"qwen2.5-coder:7b-instruct-q4_K_M\",\n",
        "        \"prompt\": \"Write a simple Python function to add two numbers.\"\n",
        "    }\n",
        "\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_endpoint, headers=headers, data=json.dumps(payload), timeout=120)\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "\n",
        "        print(f\"Status Code: {response.status_code}\")\n",
        "\n",
        "        full_response_content = \"\"\n",
        "        generated_text = \"\"\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                decoded_line = line.decode('utf-8')\n",
        "                full_response_content += decoded_line + '\\n'\n",
        "                try:\n",
        "                    json_response = json.loads(decoded_line)\n",
        "                    if 'response' in json_response:\n",
        "                        generated_text += json_response['response']\n",
        "                    if json_response.get('done'):\n",
        "                        break\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Could not decode JSON from line: {decoded_line}\")\n",
        "\n",
        "        print(\"\\n--- Full Ollama API Response (parsed line by line) ---\")\n",
        "        print(full_response_content)\n",
        "        print(\"-----------------------------------------------------\")\n",
        "\n",
        "        if generated_text:\n",
        "            print(\"\\n--- Extracted Generated Text ---\")\n",
        "            print(generated_text)\n",
        "            print(\"----------------------------------\")\n",
        "        else:\n",
        "            print(\"\\nNo generated text found in the response.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error connecting to Ollama API: {e}\")\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            print(f\"Response content: {e.response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "801875f8"
      },
      "source": [
        "### 9. Example Ollama API Call (from External)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca84f269"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "print('Executing Ollama API call to the tunneled URL:')\n",
        "\n",
        "public_url = os.environ.get('OLLAMA_PUBLIC_URL')\n",
        "\n",
        "if not public_url:\n",
        "    print('Error: OLLAMA_PUBLIC_URL not found in environment variables. Please ensure the tunnel was successfully created.')\n",
        "else:\n",
        "    ollama_api_url = f\"{public_url}/api/generate\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"qwen2.5-coder:7b-instruct-q4_K_M\",\n",
        "        \"prompt\": \"Write a js code to add two number\",\n",
        "        \"stream\": True\n",
        "    }\n",
        "\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    print(f\"Sending request to: {ollama_api_url}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.post(ollama_api_url, headers=headers, data=json.dumps(payload), stream=True, timeout=120)\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "\n",
        "        print(\"\\nGenerated Code:\")\n",
        "        generated_text = \"\"\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                decoded_line = line.decode('utf-8')\n",
        "                try:\n",
        "                    json_response = json.loads(decoded_line)\n",
        "                    if 'response' in json_response:\n",
        "                        generated_text += json_response['response']\n",
        "                    if json_response.get('done'):\n",
        "                        break\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Could not decode JSON from line: {decoded_line}\")\n",
        "        print(generated_text)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error calling Ollama API: {e}\")\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            print(f\"Response content: {e.response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}