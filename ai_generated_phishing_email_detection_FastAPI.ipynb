{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJv4JiKpIFrwtkpR7Nxvgc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushalshah0/colab_tools/blob/main/ai_generated_phishing_email_detection_FastAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhFEWqxz45jZ",
        "outputId": "0351b4ed-1447-4b57-b5ba-3a64de636722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Dependencies\n",
        "!pip install fastapi uvicorn pyngrok transformers torch tensorflow pickle-mixin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLhW16zv6odD",
        "outputId": "1acc365d-7dae-4d99-c1e7-3aa3545e3f30"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.40.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: pickle-mixin in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Verify model paths\n",
        "import os\n",
        "\n",
        "SAMPLE_NAME = \"sample1\"\n",
        "\n",
        "BASE_PATH = f\"/content/drive/MyDrive/Detect_AI_Phishing_Project/{SAMPLE_NAME}\"\n",
        "\n",
        "paths = {\n",
        "    \"LSTM\": f\"{BASE_PATH}/lstm_model.pt\",\n",
        "    \"GRU\": f\"{BASE_PATH}/gru_model.pt\",\n",
        "    \"BERT\": f\"{BASE_PATH}/bert/final_model\",\n",
        "    \"Tokenizer\": f\"{BASE_PATH}/rnn_tokenizer.pkl\"\n",
        "}\n",
        "\n",
        "for k, v in paths.items():\n",
        "    print(k, \"✅\" if os.path.exists(v) else \"❌\", v)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6gU6DBw6rSJ",
        "outputId": "fe263619-394f-4ed5-e361-1751807989bf"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM ✅ /content/drive/MyDrive/Detect_AI_Phishing_Project/sample1/lstm_model.pt\n",
            "GRU ✅ /content/drive/MyDrive/Detect_AI_Phishing_Project/sample1/gru_model.pt\n",
            "BERT ✅ /content/drive/MyDrive/Detect_AI_Phishing_Project/sample1/bert/final_model\n",
            "Tokenizer ✅ /content/drive/MyDrive/Detect_AI_Phishing_Project/sample1/rnn_tokenizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create FastAPI project structure\n",
        "import os\n",
        "\n",
        "API_DIR = \"/content/api\"\n",
        "os.makedirs(API_DIR, exist_ok=True)\n",
        "\n",
        "files = [\"main.py\", \"models.py\", \"schemas.py\"]\n",
        "for f in files:\n",
        "    with open(os.path.join(API_DIR, f), \"w\") as fp:\n",
        "        fp.write(\"\")\n",
        "\n",
        "print(\"FastAPI files created:\", files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaUNNnmV6vti",
        "outputId": "e38ea0e7-7e14-4074-9b1c-494160e7fb48"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI files created: ['main.py', 'models.py', 'schemas.py']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write schemas.py\n",
        "%%writefile /content/api/schemas.py\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class EmailRequest(BaseModel):\n",
        "    text: str\n",
        "    model: str  # bert | lstm | gru\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    model: str\n",
        "    prediction: str\n",
        "    confidence: float"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUp7M0FL69HX",
        "outputId": "3d525274-889a-43f7-9f37-41f7c342f7d6"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/api/schemas.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fe90910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f59bfa1-d0a7-43b3-bb71-c4e6cf1af6a8"
      },
      "source": [
        "#@title Write models.py\n",
        "%%writefile /content/api/models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "SAMPLE_NAME = \"sample1\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/Detect_AI_Phishing_Project/{SAMPLE_NAME}\"\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "# Define LSTM Model Architecture (assuming a basic setup)\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "# Define GRU Model Architecture (assuming a basic setup)\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        _, hidden = self.gru(embedded)\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "# LOAD TOKENIZER\n",
        "with open(f\"{BASE_PATH}/rnn_tokenizer.pkl\", \"rb\") as f:\n",
        "    rnn_tokenizer = pickle.load(f)\n",
        "\n",
        "MAX_LEN = 200\n",
        "# Corrected model parameters based on checkpoint (from error message)\n",
        "MODEL_VOCAB_SIZE = 20000    # As indicated by embedding.weight shape in error\n",
        "EMBEDDING_DIM = 128         # As indicated by embedding.weight shape in error\n",
        "HIDDEN_DIM = 128            # As indicated by lstm.weight_ih_l0 shape in error\n",
        "OUTPUT_DIM = 1              # Binary classification\n",
        "\n",
        "# LOAD LSTM\n",
        "lstm_model = LSTMModel(MODEL_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(DEVICE)\n",
        "lstm_model.load_state_dict(torch.load(f\"{BASE_PATH}/lstm_model.pt\", map_location=DEVICE))\n",
        "lstm_model.eval()\n",
        "\n",
        "# LOAD GRU\n",
        "gru_model = GRUModel(MODEL_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(DEVICE)\n",
        "gru_model.load_state_dict(torch.load(f\"{BASE_PATH}/gru_model.pt\", map_location=DEVICE))\n",
        "gru_model.eval()\n",
        "\n",
        "# LOAD BERT\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "    f\"{BASE_PATH}/bert/final_model\"\n",
        ")\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    f\"{BASE_PATH}/bert/final_model\"\n",
        ").to(DEVICE)\n",
        "bert_model.eval()\n",
        "\n",
        "# HELPERS\n",
        "def preprocess_rnn(text):\n",
        "    seq = rnn_tokenizer.texts_to_sequences([text])\n",
        "    # Map token IDs >= MODEL_VOCAB_SIZE to 0 (assuming 0 is OOV/padding)\n",
        "    processed_seq = [[token_id if token_id < MODEL_VOCAB_SIZE else 0 for token_id in s] for s in seq]\n",
        "\n",
        "    padded = np.zeros((1, MAX_LEN))\n",
        "    # Ensure sequence is not longer than MAX_LEN\n",
        "    padded[0, :min(MAX_LEN, len(processed_seq[0]))] = processed_seq[0][:MAX_LEN]\n",
        "    return torch.tensor(padded, dtype=torch.long)\n",
        "\n",
        "def predict_rnn(model, text):\n",
        "    with torch.no_grad():\n",
        "        x = preprocess_rnn(text)\n",
        "        output = model(x)\n",
        "        prob = torch.sigmoid(output).item()\n",
        "        label = \"Phishing\" if prob >= 0.5 else \"Legitimate\"\n",
        "        confidence = prob if prob >= 0.5 else 1 - prob\n",
        "        return label, confidence\n",
        "\n",
        "def predict_bert(text):\n",
        "    inputs = bert_tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512\n",
        "    ).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "        probs = torch.softmax(outputs.logits, dim=1)\n",
        "        conf, pred = torch.max(probs, dim=1)\n",
        "        label = \"Phishing\" if pred.item() == 1 else \"Legitimate\"\n",
        "        return label, conf.item()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/api/models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write main.py\n",
        "%%writefile /content/api/main.py\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from schemas import EmailRequest, PredictionResponse\n",
        "from models import predict_rnn, predict_bert, lstm_model, gru_model\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"AI-Generated Phishing Detection API\",\n",
        "    version=\"1.0\"\n",
        ")\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "def predict(request: EmailRequest):\n",
        "    text = request.text\n",
        "    model_name = request.model.lower()\n",
        "\n",
        "    if model_name == \"bert\":\n",
        "        label, confidence = predict_bert(text)\n",
        "\n",
        "    elif model_name == \"lstm\":\n",
        "        label, confidence = predict_rnn(lstm_model, text)\n",
        "\n",
        "    elif model_name == \"gru\":\n",
        "        label, confidence = predict_rnn(gru_model, text)\n",
        "\n",
        "    else:\n",
        "        raise HTTPException(\n",
        "            status_code=400,\n",
        "            detail=\"Invalid model. Choose from: bert, lstm, gru\"\n",
        "        )\n",
        "\n",
        "    return PredictionResponse(\n",
        "        model=model_name,\n",
        "        prediction=label,\n",
        "        confidence=round(confidence, 4)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2emq9sJ-7Nig",
        "outputId": "d6b2a408-d3d4-4db0-bb8a-2670d731569b"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/api/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b269a6c",
        "outputId": "23a4f9be-5e10-4fe6-a018-8f4ad418dacc"
      },
      "source": [
        "#@title Run FastAPI\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Kill any processes running on port 8000 or any uvicorn process\n",
        "!pkill -f uvicorn || true\n",
        "!fuser -k 8000/tcp || true\n",
        "\n",
        "%cd /content/api\n",
        "\n",
        "# Start uvicorn in the background using nohup\n",
        "!nohup uvicorn main:app --host 0.0.0.0 --port 8000 > nohup.out 2>&1 &\n",
        "\n",
        "print(\"FastAPI server starting in the background...\")\n",
        "time.sleep(5) # Give it some time to start\n",
        "print(\"FastAPI server should be running.\")"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "/content/api\n",
            "FastAPI server starting in the background...\n",
            "FastAPI server should be running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "994520c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38983bba-0560-4384-b0ca-16f561198aa9"
      },
      "source": [
        "#@title Display FastAPI Server Logs\n",
        "!cat nohup.out"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-20 18:07:31.955610: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-01-20 18:07:31.960874: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-01-20 18:07:31.975648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768932452.002328   27993 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768932452.010904   27993 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768932452.029848   27993 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768932452.029897   27993 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768932452.029899   27993 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768932452.029902   27993 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-20 18:07:32.035421: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "INFO:     Started server process [27993]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adbb8f78",
        "outputId": "2791c8c7-71da-4d0a-dd8b-babee541aa25"
      },
      "source": [
        "#@title Test API Locally (with model selector)\n",
        "import requests\n",
        "\n",
        "API_URL = \"http://127.0.0.1:8000/predict\"\n",
        "\n",
        "email_text = \"Dear Customer,   Your bank account has been temporarily suspended due to suspicious activity.  Please click the link below to verify your account immediately:  http://verify-bank-login.com\" #@param {type:\"string\"}\n",
        "model_selector = \"bert\" #@param [\"bert\", \"lstm\", \"gru\"]\n",
        "\n",
        "payload = {\n",
        "    \"text\": email_text,\n",
        "    \"model\": model_selector\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, json=payload)\n",
        "\n",
        "print(\"Status Code:\", response.status_code)\n",
        "print(\"Response:\", response.json())"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status Code: 200\n",
            "Response: {'model': 'bert', 'prediction': 'Phishing', 'confidence': 0.9971}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Cloudflare Tunnel\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "!cloudflared --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyw5HkL_fl_k",
        "outputId": "558b1f2e-675e-4251-9657-7c746d31fcf0"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-20 18:08:10--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2026.1.1/cloudflared-linux-amd64.deb [following]\n",
            "--2026-01-20 18:08:10--  https://github.com/cloudflare/cloudflared/releases/download/2026.1.1/cloudflared-linux-amd64.deb\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/106867604/12aae843-3db0-4414-9b56-e2a442db3e76?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-01-20T18%3A53%3A46Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64.deb&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-01-20T17%3A53%3A03Z&ske=2026-01-20T18%3A53%3A46Z&sks=b&skv=2018-11-09&sig=GbOPm%2FDnqwqPGRnyqdRloQFwEAVNL7Z177EY8Y8fMOA%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2ODkzNDI5MCwibmJmIjoxNzY4OTMyNDkwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.4VYi4z83Xx7SmWhwkQIXQQxihPdLeSZl5TvTLbjreMA&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64.deb&response-content-type=application%2Foctet-stream [following]\n",
            "--2026-01-20 18:08:10--  https://release-assets.githubusercontent.com/github-production-release-asset/106867604/12aae843-3db0-4414-9b56-e2a442db3e76?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-01-20T18%3A53%3A46Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64.deb&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-01-20T17%3A53%3A03Z&ske=2026-01-20T18%3A53%3A46Z&sks=b&skv=2018-11-09&sig=GbOPm%2FDnqwqPGRnyqdRloQFwEAVNL7Z177EY8Y8fMOA%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2ODkzNDI5MCwibmJmIjoxNzY4OTMyNDkwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.4VYi4z83Xx7SmWhwkQIXQQxihPdLeSZl5TvTLbjreMA&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64.deb&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20129572 (19M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared-linux-amd64.deb.2’\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  19.20M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2026-01-20 18:08:10 (192 MB/s) - ‘cloudflared-linux-amd64.deb.2’ saved [20129572/20129572]\n",
            "\n",
            "(Reading database ... 117532 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2026.1.1) over (2026.1.1) ...\n",
            "Setting up cloudflared (2026.1.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "cloudflared version 2026.1.1 (built 2026-01-20-11:14 UTC)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Cloudflare Tunnel\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Kill any existing cloudflared process to ensure a clean start\n",
        "!pkill -f cloudflared || true\n",
        "\n",
        "# Start cloudflared in the background using nohup\n",
        "!nohup cloudflared tunnel --url http://127.0.0.1:8000 > cloudflared.out 2>&1 &\n",
        "\n",
        "print(\"Cloudflare tunnel starting in the background...\")\n",
        "time.sleep(5) # Give it some time to start the process\n",
        "\n",
        "tunnel_url = None\n",
        "start_time = time.time()\n",
        "timeout = 60 # seconds\n",
        "\n",
        "while not tunnel_url and (time.time() - start_time < timeout):\n",
        "    if os.path.exists('cloudflared.out'):\n",
        "        with open('cloudflared.out', 'r') as f:\n",
        "            output = f.read()\n",
        "            # Regex to find the URL, assuming it's in the format https://<subdomain>.trycloudflare.com\n",
        "            match = re.search(r'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com', output)\n",
        "            if match:\n",
        "                tunnel_url = match.group(0)\n",
        "                break\n",
        "    time.sleep(2) # Check every 2 seconds\n",
        "\n",
        "if tunnel_url:\n",
        "    print(f\"Cloudflare tunnel is running. Public URL: {tunnel_url}\")\n",
        "    # Update the CLOUDFLARE_TUNNEL_URL variable for future use\n",
        "    global CLOUDFLARE_TUNNEL_URL\n",
        "    CLOUDFLARE_TUNNEL_URL = tunnel_url + '/predict'\n",
        "else:\n",
        "    print(\"Could not find Cloudflare tunnel URL within the timeout period.\")\n",
        "    print(\"Last few lines of cloudflared.out:\")\n",
        "    if os.path.exists('cloudflared.out'):\n",
        "        with open('cloudflared.out', 'r') as f:\n",
        "            print(f.readlines()[-10:])\n",
        "    else:\n",
        "        print(\"cloudflared.out not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUpLr5vafqXc",
        "outputId": "45e1d81c-b7c1-47b5-fc98-224225030325"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "Cloudflare tunnel starting in the background...\n",
            "Cloudflare tunnel is running. Public URL: https://analyst-stewart-cloth-subscribe.trycloudflare.com\n"
          ]
        }
      ]
    }
  ]
}